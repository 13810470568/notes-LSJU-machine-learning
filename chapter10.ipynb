{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第十讲：特征选择\n",
    "\n",
    "回顾一下上一讲的内容，如果有一个有限的假设类$\\lvert\\mathcal{H}\\rvert=k$，对于给定的$\\gamma,\\delta$，要保证$\\varepsilon(\\hat h)\\leq\\displaystyle\\operatorname*{min}_{h\\in\\mathcal{H}}\\varepsilon(h)+2\\gamma$成立，并且在事件“训练误差与泛化误差之差小于$\\gamma$”的概率至少为$1-\\delta$时，需要满足：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m&\\geq\\frac{1}{2\\gamma^2}\\log\\frac{2k}{\\delta}\\\\\n",
    "&=O\\left(\\frac{1}{\\gamma^2}\\log\\frac{k}{\\delta}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "这是一个与样本复杂度相关的结论。现在我们想要将这一结论推广到无限假设类中。\n",
    "\n",
    "## 4. 若$\\mathcal{H}$是无限的\n",
    "\n",
    "我们已经证明了一些在有限假设类下非常有用的理论，然而，包括“任意以实数为参数的假设函数”（如线性分类器）在内的很多的假设类实际上包含了无数个假设函数。我们能否将有限假设类下成立的结论推广到无限假设类中呢？\n",
    "\n",
    "我们先抛出一个不严谨的论点，也许在某些方面不够“正确”，但它比较直观。之后，我们会给出一个更好且更普遍的结论。\n",
    "\n",
    "假设我们要将上面关于样本复杂度的式子应用在逻辑回归中，此时我们的假设类是由线性判别边界构成的，所以$\\mathcal{H}$是以$d$个实数作为参数（如果我用逻辑回归解决$n$个特征值的问题，则$d=n+1$，此时逻辑回归会找到一个以$n+1$个实数为参数的线性判别边界$g\\left(\\theta^Tx\\right)$）。而我们都知道，计算机通常使用IEEE双精度浮点数（C语言中的`double`，$64$位）来近似实数，那么在这个逻辑回归的情况下，计算机需要$64d$个比特位（bits）来表示这些参数。那么对我们的假设类就有$k=\\lvert\\mathcal{H}\\rvert=2^{64d}$个不同的假设函数。根据上一讲最后的结论，带入$O$表达式，为了保证样本复杂度中的条件$\\varepsilon{\\hat h}\\leq\\varepsilon{h^*}+2\\gamma$的概率至少为$1-\\delta$，就需要满足\n",
    "$\n",
    "m\\geq O\\left(\\frac{1}{\\gamma^2}\\log\\frac{2^{64d}}{\\delta}\\right)\n",
    "=O\\left(\\frac{d}{\\gamma^2}\\log\\frac{1}{\\delta}\\right)\n",
    "=O_{\\gamma,\\delta}(d)\n",
    "$\n",
    "（最后一个$O$记号的下标$\\gamma,\\delta$表明复杂度可能依赖于这两个隐藏参数）。从这里可以大概了解，所需训练样本的数量与模型参数的个数大约最多呈线性关系。\n",
    "\n",
    "虽然使用$64$位浮点数来推断这个结论并不能完全令人信服，但是这个结论大体上是正确的。如果要保证经验风险最小化生成的假设与最好假设间泛化误差的差距不超过$2\\gamma$，则$m$必须大于大$O$记号中的量级，如果我们尝试最小化训练误差，为了保证学习效果我们使用了含有$d$个参数的假设类，则所需要的训练样本的数量大致与我们假设类的参数数目呈线性关系，即$m$与$d$大致呈线性关系。\n",
    "\n",
    "（到目前为止，这些结论并不能说明一个使用经验风险最小化的算法的价值。因此，虽然多数尝试最小化或估计训练误差的判别学习算法对参数个数$d$为线性复杂度，但这个结论并不是对所有判别学习算法有效。对很多非经验风险最小化使用良好的假设保证仍是热门研究方向。）\n",
    "\n",
    "上面推导出的结论还有一个不那么令人满意的地方——它依赖参数化$\\mathcal{H}$。虽然直观上不容易看出问题，我们将具有$n+1$个参数$\\theta_0,\\cdots,\\theta_n$线性分类器所在的类写作$1\\left\\{\\theta_0^T+\\theta_1^Tx_1+\\cdots+\\theta_n^Tx_n\\geq0\\right\\}$，我们也可以将其写作$h_{u,v}(x)=1\\left\\{\\left(u_0^2-v_0^2\\right)+\\left(u_1^2-v_1^2\\right)x_1+\\cdots+\\left(u_n^2-v_n^2\\right)x_n\\geq0\\right\\}$，此时有$2n+2$个参数。不过，这两个式子表达的是同一个假设类$\\mathcal{H}$，即$n$维线性分类器的集合。\n",
    "\n",
    "为了给出一个更严谨的推导，我们先定义一些新概念。\n",
    "\n",
    "对于某个由点$x^{(i)}\\in\\mathcal{X}$构成的集合$S=\\left\\{x^{(1)},\\cdots,x^{(d)}\\right\\}$（并不要求是训练集），如果$\\mathcal{H}$可以实现$S$任意一种标记方式，即对于任意集合$\\left\\{y^{(1)},\\cdots,y^{(d)}\\right\\}$，存在$h\\in\\mathcal{H}$能够使$h\\left(x^{(i)}\\right)=y^{(i)},\\ i=1,\\cdots,d$成立，我们就称$\\mathcal{H}$**分散了（shatters）**$S$。\n",
    "\n",
    "对于给定假设类$\\mathcal{H}$，我们定义**Vapnik-Chervonenkis维度（Vapnik-Chervonenkis dimension）**，写作$\\mathrm{VC}(\\mathcal{H})$，表示能够被$\\mathcal{H}$分散的最大的集合。（如果$\\mathcal{H}$可以分散任意大小的集合，则有$\\mathrm{VC}(\\mathcal{H})=\\infty$）\n",
    "\n",
    "举个例子，考虑下图中的点：\n",
    "\n",
    "<img src=\"./resource/chapter10_image01.png\" width=\"250\" alt=\"\" align=center />\n",
    "\n",
    "在两个维度上的（$h(x)=1\\left\\{\\theta_0+\\theta_1x_1+\\theta_2x_2\\geq0\\right\\}$）线性分类器的集合$\\mathcal{H}$可以分散上面的点集吗？答案是肯定的。我们可以直接给出所有可能的八种分类标记的情况，而且对这个例子我们总是能够找到一个具有零训练误差的线性分类器（即完美分类）：\n",
    "\n",
    "<img src=\"./resource/chapter10_image02.png\" width=\"800\" alt=\"\" align=center />\n",
    "\n",
    "（对于上例的点集$S$，所有可能的分类情况都有对应的二维分类器将点分散）\n",
    "\n",
    "此外，通过一系列计算能够证明，此假设类（即二维线性分类器）不可能分开任意含有$4$个点的集合（不论这四个点在$x_1,x_2$坐标系上如何排列，我们都无法找到一组能够将“四个点的所有可能的$x,o$的标记情况”分开的直线）。因此，$\\mathcal{H}$可以分开的集合的元素总数为$3$，有$\\mathrm{VC}(\\mathcal{H})=3$。\n",
    "\n",
    "值得注意的是，虽然$\\mathcal{H}$的$\\mathrm{VC}$维是$3$，但也存在分不开的布局。比如，如果三点共线（如下方左图所示），则存在二维线性分类器无法分开的标记情况（如下方右图所示）：\n",
    "\n",
    "<img src=\"./resource/chapter10_image03.png\" width=\"600\" alt=\"\" align=center />\n",
    "\n",
    "（因为已经存在某个$3$个点的集合可以被二维线性分类器分散，所以这并不影响二维线性分类器的$\\mathrm{VC}$维为$3$。）\n",
    "\n",
    "也就是说，从$\\mathrm{VC}$维的定义有，为了证明$\\mathrm{VC}(\\mathcal{H})$至少是$d$，那么我们只需要找到一个大小为$d$的集合可以被$\\mathcal{H}$散列即可。\n",
    "\n",
    "**定理**，给定$\\mathcal{H}$，令$d=\\mathrm{VC}(\\mathcal{H})$，在保证事件“训练误差与泛化误差在$\\gamma$以内”的概率至少为$1-\\delta$的条件下，对于$h\\in\\mathcal{H}$有：\n",
    "\n",
    "$$\n",
    "\\left\\lvert\\varepsilon(h)-\\hat\\varepsilon(h)\\right\\rvert\\leq O\\left(\\sqrt{\\frac{d}{m}\\log\\frac{m}{d}+\\frac{1}{m}\\log\\frac{1}{\\delta}}\\right)\n",
    "$$\n",
    "\n",
    "因此，在概率至少为$1-\\delta$时可以得到：\n",
    "\n",
    "$$\n",
    "\\varepsilon\\left(\\hat h\\right)\\leq\\varepsilon\\left(h^*\\right)+O\\left(\\sqrt{\\frac{d}{m}\\log\\frac{m}{d}+\\frac{1}{m}\\log\\frac{1}{\\delta}}\\right)\n",
    "$$\n",
    "\n",
    "换句话说，如果一个假设类具有有限的$\\mathrm{VC}$维度，则当$m$越来越大时会产生一致收敛。同上一讲一样，我们可以从中得到一个使用$\\varepsilon\\left(h^*\\right)$表示的$\\varepsilon(h)$的界限。我们还能够得到下面的推论：\n",
    "\n",
    "**推论**，对于$h\\in\\mathcal{H}$（由于$\\varepsilon\\left(\\hat h\\right)\\leq\\varepsilon\\left(h^*\\right)+2\\gamma$），要保证$\\left\\lvert\\varepsilon(h)-\\hat\\varepsilon(h)\\right\\rvert\\leq\\gamma$的概率至少为$1-\\delta$，则必须满足$m=O_{\\gamma,\\delta}(d)$。\n",
    "\n",
    "也就是说，要保证从$\\mathcal{H}$中选取的假设能够“学的好”，所需的训练样本的个数与$\\mathcal{H}$的$\\mathrm{VC}$维数呈线性关系。这个结论也可以解释为，对于“大多数”假设类（我们假设”合理的“参数化），其$\\mathrm{VC}$维与其参数个数呈大致的线性关系（其实这两者的大小通常差不多，比如使用维度为$n$逻辑回归作为线性分类器时，一共需要$n+1$个参数，而$n$维线性分类器的假设类的$\\mathrm{VC}$维是$n+1$）。综合上面的各结论有，（对于一个尝试最小化训练误差）算法所需的训练样本的数量通常与假设类$\\mathcal{H}$参数的个数呈大致上的线性关系。这也表明了样本复杂度的上下界都是由$\\mathrm{VC}$维给出的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\left(\\right)\n",
    "\\left\\lvert\\right\\rvert\n",
    "\\mathrm{VC}(\\mathcal{H})\n",
    "\\begin{align}\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
